{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "948b95af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertModel, DistilBertTokenizer\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c78ee775",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Extract text from file names as well as labels\n",
    "def read_text_files_with_labels(path):\n",
    "    texts = []\n",
    "    labels = []\n",
    "    class_folders = sorted(os.listdir(path))  # Assuming class folders are sorted\n",
    "    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n",
    "\n",
    "    for class_name in class_folders:\n",
    "        class_path = os.path.join(path, class_name)\n",
    "        if os.path.isdir(class_path):\n",
    "            file_names = os.listdir(class_path)\n",
    "            for file_name in file_names:\n",
    "                file_path = os.path.join(class_path, file_name)\n",
    "                if os.path.isfile(file_path):\n",
    "                    file_name_no_ext, _ = os.path.splitext(file_name)\n",
    "                    text = file_name_no_ext.replace('_', ' ')\n",
    "                    text_without_digits = re.sub(r'\\d+', '', text)\n",
    "                    texts.append(text_without_digits)\n",
    "                    labels.append(label_map[class_name])\n",
    "\n",
    "    return np.array(texts), np.array(labels)\n",
    "\n",
    "# Define your dataset class\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_len):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'text': text,\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Define the model\n",
    "\n",
    "\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\"\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            self.distilbert.config.hidden_size,\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "        # Freeze ALL DistilBERT layers initially\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        x = self.dropout(cls_output)\n",
    "\n",
    "        return self.classifier(x)\n",
    "    \n",
    "# Define training function\n",
    "def train(model, iterator, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in iterator:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(input_ids, attention_mask)\n",
    "        loss = criterion(output, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, iterator, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in iterator:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            output = model(input_ids, attention_mask)\n",
    "            loss = criterion(output, labels)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(iterator)\n",
    "\n",
    "def predict(model, dataloader, device):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    predictions = []\n",
    "    with torch.no_grad():  # Disable gradient tracking\n",
    "        for batch in dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)  # Assuming input_ids are in the batch\n",
    "            attention_mask = batch['attention_mask'].to(device)  # Assuming attention_mask is in the batch\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            # Get predictions\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "\n",
    "            # Convert predictions to CPU and append to the list\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "528d5404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11629,)\n",
      "(11629,)\n",
      "(1800,)\n",
      "(1800,)\n",
      "(3432,)\n",
      "(3432,)\n"
     ]
    }
   ],
   "source": [
    "TRAIN_PATH = r\"C:\\Users\\john2\\Desktop\\uofc\\617\\assignment2\\garbage_data\\garbage_data\\CVPR_2024_dataset_Train\"\n",
    "VAL_PATH = r\"C:\\Users\\john2\\Desktop\\uofc\\617\\assignment2\\garbage_data\\garbage_data\\CVPR_2024_dataset_Val\"\n",
    "TEST_PATH = r\"C:\\Users\\john2\\Desktop\\uofc\\617\\assignment2\\garbage_data\\garbage_data\\CVPR_2024_dataset_Test\"\n",
    "\n",
    "text_train,labels_train = read_text_files_with_labels(TRAIN_PATH)\n",
    "text_val,labels_val = read_text_files_with_labels(VAL_PATH)\n",
    "text_test,labels_test = read_text_files_with_labels(TEST_PATH)\n",
    "\n",
    "print(text_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(text_val.shape)\n",
    "print(labels_val.shape)\n",
    "print(text_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "51deaefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/6\n",
      "Training classifier only\n",
      "Train Loss: 1.3215\n",
      "Val Loss: 1.2317\n",
      "✅ Model saved\n",
      "\n",
      "Epoch 2/6\n",
      "Train Loss: 1.2163\n",
      "Val Loss: 1.1513\n",
      "✅ Model saved\n",
      "\n",
      "Epoch 3/6\n",
      "Unfreezing last 2 transformer layers\n",
      "Train Loss: 0.4987\n",
      "Val Loss: 0.3636\n",
      "✅ Model saved\n",
      "\n",
      "Epoch 4/6\n",
      "Train Loss: 0.3520\n",
      "Val Loss: 0.3254\n",
      "✅ Model saved\n",
      "\n",
      "Epoch 5/6\n",
      "Unfreezing last 4 transformer layers\n",
      "Train Loss: 0.3099\n",
      "Val Loss: 0.3136\n",
      "✅ Model saved\n",
      "\n",
      "Epoch 6/6\n",
      "Unfreezing last 4 transformer layers\n",
      "Train Loss: 0.2677\n",
      "Val Loss: 0.3126\n",
      "✅ Model saved\n"
     ]
    }
   ],
   "source": [
    "# Tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize data\n",
    "max_len = 24\n",
    "dataset_train = CustomDataset(text_train, labels_train, tokenizer, max_len)\n",
    "dataset_val = CustomDataset(text_val, labels_val, tokenizer, max_len)\n",
    "dataset_test = CustomDataset(text_test, labels_test, tokenizer, max_len)\n",
    "\n",
    "# Data loaders\n",
    "train_loader = DataLoader(dataset_train, batch_size=8, shuffle=True)\n",
    "val_loader = DataLoader(dataset_val, batch_size=8, shuffle=False)\n",
    "test_loader = DataLoader(dataset_test, batch_size=8, shuffle=False)\n",
    "\n",
    "\n",
    "# Model\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = DistilBERTClassifier(num_classes=4).to(device)\n",
    "\n",
    "# Training parameters\n",
    "optimizer = optim.AdamW(model.parameters(), lr=2e-5, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_loss = float(\"inf\")\n",
    "EPOCHS = 6\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    print(f\"\\nEpoch {epoch+1}/{EPOCHS}\")\n",
    "\n",
    "    # Stage freezing logic\n",
    "    if epoch == 0:\n",
    "        print(\"Training classifier only\")\n",
    "\n",
    "    elif epoch == 2:\n",
    "        print(\"Unfreezing last 2 transformer layers\")\n",
    "        for layer in model.distilbert.transformer.layer[-2:]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5\n",
    "        )\n",
    "\n",
    "    elif epoch == 4:\n",
    "        print(\"Unfreezing last 4 transformer layers\")\n",
    "        for layer in model.distilbert.transformer.layer[-4:-2]:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5\n",
    "        )\n",
    "    elif epoch == 5:\n",
    "        print(\"Unfreezing last 4 transformer layers\")\n",
    "        for layer in model.distilbert.transformer.layer:\n",
    "            for param in layer.parameters():\n",
    "                param.requires_grad = True\n",
    "        optimizer = optim.AdamW(\n",
    "            filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5\n",
    "        )\n",
    "\n",
    "    ####################\n",
    "    # TRAIN\n",
    "    ####################\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for batch in train_loader:\n",
    "\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    train_loss /= len(train_loader)\n",
    "\n",
    "    ####################\n",
    "    # VALIDATE\n",
    "    ####################\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_loss /= len(val_loader)\n",
    "\n",
    "    print(f\"Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "    ####################\n",
    "    # SAVE BEST MODEL\n",
    "    ####################\n",
    "    if val_loss < best_loss:\n",
    "\n",
    "        best_loss = val_loss\n",
    "\n",
    "        torch.save(\n",
    "            {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"val_loss\": val_loss,\n",
    "            },\n",
    "            \"best_model.pt\",\n",
    "        )\n",
    "\n",
    "        print(\"✅ Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f3395d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8531\n"
     ]
    }
   ],
   "source": [
    "# Load checkpoint\n",
    "checkpoint = torch.load('best_model.pt', map_location=device)\n",
    "\n",
    "# Load weights into model\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "\n",
    "# Run prediction\n",
    "test_predictions = predict(model, test_loader, device)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = (test_predictions == labels_test).sum() / len(labels_test)\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a78b7b2f",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'confusion_matrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m cm \u001b[38;5;241m=\u001b[39m confusion_matrix(labels_test, test_predictions)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Plot confusion matrix\u001b[39;00m\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'confusion_matrix' is not defined"
     ]
    }
   ],
   "source": [
    "cm = confusion_matrix(labels_test, test_predictions)\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, cmap='Blues', fmt='g', cbar=False)\n",
    "\n",
    "plt.title('Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
