{
  "metadata": {
    "kernelspec": {
      "name": "",
      "display_name": ""
    },
    "language_info": {
      "name": ""
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "95e05ddb-678d-4117-a563-6071b9d37021",
      "cell_type": "code",
      "source": "\"\"\"\nComplete Multimodal Garbage Classification System\nCombines DistilBERT (text) and ResNet18 (image) models for robust classification\nBased on the provided notebook implementations\n\"\"\"\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.optim.lr_scheduler import ExponentialLR\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom torchvision import transforms, models\nfrom torchvision.datasets import ImageFolder\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport os\nimport re\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport warnings\nwarnings.filterwarnings('ignore')\n#from google.colab import drive\n#drive.mount('/content/drive')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7a2c1baa-c4f2-4f68-a757-419cb6ec6eca",
      "cell_type": "code",
      "source": "# ============================================================================\n# CONFIGURATION\n# ============================================================================\n\n# Update these paths to your dataset location\n#TRAIN_PATH = \"/content/drive/MyDrive/CVPR_2024_dataset_Train\"\n#VAL_PATH = \"/content/drive/MyDrive/CVPR_2024_dataset_Val\"\n#TEST_PATH = \"/content/drive/MyDrive/CVPR_2024_dataset_Test\"\n\ndata_dir = \"/work/TALC/ensf617_2026w/garbage_data\"\nTRAIN_PATH = os.path.join(data_dir, \"CVPR_2024_dataset_Train\")\nVAL_PATH   = os.path.join(data_dir, \"CVPR_2024_dataset_Val\")\nTEST_PATH  = os.path.join(data_dir, \"CVPR_2024_dataset_Test\")\n\n# Model save paths\nTEXT_MODEL_PATH = 'best_text_model.pth'\nIMAGE_MODEL_PATH = 'best_image_model.pth'\n\n# Hyperparameters\nTEXT_MAX_LEN = 24\nTEXT_BATCH_SIZE = 8\nTEXT_EPOCHS = 4\nTEXT_LR = 2e-5\n\nIMAGE_BATCH_SIZE = 32\nIMAGE_EPOCHS = 20\nIMAGE_LR = 0.001\n\nNUM_CLASSES = 4\nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nprint(f\"Using device: {DEVICE}\")",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c89ae51e-229e-491b-a958-e7bcb9dce508",
      "cell_type": "code",
      "source": "# ============================================================================\n# PART 1: TEXT MODEL COMPONENTS\n# ============================================================================\n\ndef read_text_files_with_labels(path):\n    \"\"\"Extract text from filenames and labels from folder structure\"\"\"\n    texts = []\n    labels = []\n    class_folders = sorted([\n         d for d in os.listdir(path)\n         if os.path.isdir(os.path.join(path, d))\n    ])\n    label_map = {class_name: idx for idx, class_name in enumerate(class_folders)}\n\n    for class_name in class_folders:\n        class_path = os.path.join(path, class_name)\n        if os.path.isdir(class_path):\n            file_names = sorted(os.listdir(class_path))\n            for file_name in file_names:\n                file_path = os.path.join(class_path, file_name)\n                if os.path.isfile(file_path):\n                    file_name_no_ext, _ = os.path.splitext(file_name)\n                    text = file_name_no_ext.replace('_', ' ')\n                    text_without_digits = re.sub(r'\\d+', '', text)\n                    texts.append(text_without_digits)\n                    labels.append(label_map[class_name])\n\n    return np.array(texts), np.array(labels), label_map\n\n\nclass TextDataset(Dataset):\n    \"\"\"Dataset for text classification\"\"\"\n    def __init__(self, texts, labels, tokenizer, max_len):\n        self.texts = texts\n        self.labels = labels\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, idx):\n        text = str(self.texts[idx])\n        label = self.labels[idx]\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'text': text,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long)\n        }\n\n\nclass DistilBERTClassifier(nn.Module):\n    \"\"\"Text classifier using DistilBERT\"\"\"\n    def __init__(self, num_classes):\n        super(DistilBERTClassifier, self).__init__()\n        self.distilbert = DistilBertModel.from_pretrained('distilbert-base-uncased')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(self.distilbert.config.hidden_size, num_classes)\n\n    def forward(self, input_ids, attention_mask):\n        pooled_output = self.distilbert(input_ids=input_ids, attention_mask=attention_mask)[0]\n        output = self.drop(pooled_output[:, 0])\n        return self.out(output)\n\n\ndef train_text_epoch(model, iterator, optimizer, criterion, device):\n    \"\"\"Train text model for one epoch\"\"\"\n    model.train()\n    total_loss = 0\n    for batch in iterator:\n        input_ids = batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['label'].to(device)\n\n        optimizer.zero_grad()\n        output = model(input_ids, attention_mask)\n        loss = criterion(output, labels)\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    return total_loss / len(iterator)\n\n\ndef evaluate_text_model(model, iterator, criterion, device):\n    \"\"\"Evaluate text model\"\"\"\n    model.eval()\n    total_loss = 0\n    with torch.no_grad():\n        for batch in iterator:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            labels = batch['label'].to(device)\n\n            output = model(input_ids, attention_mask)\n            loss = criterion(output, labels)\n            total_loss += loss.item()\n\n    return total_loss / len(iterator)\n\n\ndef predict_text(model, dataloader, device):\n    \"\"\"Get predictions from text model\"\"\"\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for batch in dataloader:\n            input_ids = batch['input_ids'].to(device)\n            attention_mask = batch['attention_mask'].to(device)\n            outputs = model(input_ids, attention_mask)\n            _, preds = torch.max(outputs, dim=1)\n            predictions.extend(preds.cpu().numpy())\n    return predictions\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "30a5782c-46ba-471b-bc79-c7d59936d759",
      "cell_type": "code",
      "source": "# ============================================================================\n# PART 2: IMAGE MODEL COMPONENTS\n# ============================================================================\n\nclass GarbageImageModel(nn.Module):\n    \"\"\"Image classifier using ResNet18 with transfer learning\"\"\"\n    def __init__(self, num_classes, transfer=True):\n        super().__init__()\n\n        # Load pretrained or random\n        if transfer:\n            self.feature_extractor = models.resnet18(weights='IMAGENET1K_V1')\n        else:\n            self.feature_extractor = models.resnet18(weights=None)\n\n        # Get feature size\n        num_features = self.feature_extractor.fc.in_features\n\n        # Remove original classifier\n        self.feature_extractor.fc = nn.Identity()\n\n        # Freeze backbone if transfer learning\n        if transfer:\n            for param in self.feature_extractor.parameters():\n                param.requires_grad = False\n\n        # New classifier\n        self.classifier = nn.Linear(num_features, num_classes)\n\n    def forward(self, x):\n        x = self.feature_extractor(x)\n        x = self.classifier(x)\n        return x\n\n\ndef train_image_model(model, trainloader, valloader, criterion, optimizer, \n                      scheduler, device, epochs, save_path):\n    \"\"\"Train image model\"\"\"\n    best_loss = 1e+20\n    \n    for epoch in range(epochs):\n        # Training\n        model.train()\n        train_loss = 0.0\n        \n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data[0].to(device), data[1].to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            train_loss += loss.item()\n        \n        avg_train_loss = train_loss / (i + 1)\n        print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {avg_train_loss:.3f}', end=' ')\n        scheduler.step()\n        \n        # Validation\n        model.eval()\n        val_loss = 0\n        with torch.no_grad():\n            for i, data in enumerate(valloader, 0):\n                inputs, labels = data[0].to(device), data[1].to(device)\n                outputs = model(inputs)\n                loss = criterion(outputs, labels)\n                val_loss += loss.item()\n        \n        avg_val_loss = val_loss / (i + 1)\n        print(f'Val Loss: {avg_val_loss:.3f}')\n        \n        # Save best model\n        if val_loss < best_loss:\n            print(\"  -> Saving best model\")\n            torch.save(model.state_dict(), save_path)\n            best_loss = val_loss\n    \n    print('Finished Training Image Model')\n    return model\n\n\ndef predict_image(model, dataloader, device):\n    \"\"\"Get predictions from image model\"\"\"\n    model.eval()\n    predictions = []\n    with torch.no_grad():\n        for data in dataloader:\n            images, _ = data\n            images = images.to(device)\n            outputs = model(images)\n            _, preds = torch.max(outputs, 1)\n            predictions.extend(preds.cpu().numpy())\n    return predictions\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "ff55ae65-4e09-4eac-a971-04b5eadae844",
      "cell_type": "code",
      "source": "# ============================================================================\n# PART 3: MULTIMODAL COMPONENTS\n# ============================================================================\n\nclass MultimodalDataset(Dataset):\n    \"\"\"Dataset that provides both image and text for each sample\"\"\"\n    def __init__(self, image_folder_dataset, texts, tokenizer, max_len):\n        self.image_dataset = image_folder_dataset\n        self.texts = texts\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.image_dataset)\n\n    def __getitem__(self, idx):\n        image, label = self.image_dataset[idx]\n        text = str(self.texts[idx])\n\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n\n        return {\n            'image': image,\n            'input_ids': encoding['input_ids'].flatten(),\n            'attention_mask': encoding['attention_mask'].flatten(),\n            'label': torch.tensor(label, dtype=torch.long),\n            'text': text\n        }\n\n\nclass MultimodalGarbageClassifier:\n    \"\"\"Multimodal classifier combining text and image models\"\"\"\n    def __init__(self, text_model, image_model, tokenizer, transform, \n                 device, class_names):\n        self.text_model = text_model\n        self.image_model = image_model\n        self.tokenizer = tokenizer\n        self.transform = transform\n        self.device = device\n        self.class_names = class_names\n        \n        self.text_model.eval()\n        self.image_model.eval()\n    \n    def classify_from_path(self, image_path, text, alpha=0.5, return_details=False):\n        \"\"\"\n        Classify a single image with text description\n        \n        Args:\n            image_path: Path to image file\n            text: Text description (e.g., from filename)\n            alpha: Weight for text model (1-alpha for image model)\n            return_details: If True, return detailed probabilities\n        \n        Returns:\n            predicted_class, confidence, (optional) text_probs, image_probs\n        \"\"\"\n        # Text prediction\n        encoding = self.tokenizer(\n            text,\n            add_special_tokens=True,\n            max_length=TEXT_MAX_LEN,\n            return_token_type_ids=False,\n            padding='max_length',\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        \n        input_ids = encoding['input_ids'].to(self.device)\n        attention_mask = encoding['attention_mask'].to(self.device)\n        \n        with torch.no_grad():\n            text_logits = self.text_model(input_ids, attention_mask)\n            text_probs = F.softmax(text_logits, dim=1)\n        \n        # Image prediction\n        image = Image.open(image_path).convert('RGB')\n        image_tensor = self.transform(image).unsqueeze(0).to(self.device)\n        \n        with torch.no_grad():\n            image_logits = self.image_model(image_tensor)\n            image_probs = F.softmax(image_logits, dim=1)\n        \n        # Combine predictions\n        combined_probs = alpha * text_probs + (1 - alpha) * image_probs\n        predicted_class = torch.argmax(combined_probs, dim=1).item()\n        confidence = combined_probs[0][predicted_class].item()\n        \n        if return_details:\n            return predicted_class, confidence, text_probs[0].cpu().numpy(), image_probs[0].cpu().numpy()\n        return predicted_class, confidence\n    \n    def classify_batch(self, dataloader, alpha=0.5):\n        \"\"\"\n        Classify a batch of samples from MultimodalDataset\n        \n        Args:\n            dataloader: DataLoader with MultimodalDataset\n            alpha: Weight for text model\n            \n        Returns:\n            predictions, labels, text_probs, image_probs\n        \"\"\"\n        all_predictions = []\n        all_labels = []\n        all_text_probs = []\n        all_image_probs = []\n        \n        self.text_model.eval()\n        self.image_model.eval()\n        \n        with torch.no_grad():\n            for batch in dataloader:\n                images = batch['image'].to(self.device)\n                input_ids = batch['input_ids'].to(self.device)\n                attention_mask = batch['attention_mask'].to(self.device)\n                labels = batch['label'].to(self.device)\n                \n                # Text predictions\n                text_logits = self.text_model(input_ids, attention_mask)\n                text_probs = F.softmax(text_logits, dim=1)\n                \n                # Image predictions\n                image_logits = self.image_model(images)\n                image_probs = F.softmax(image_logits, dim=1)\n                \n                # Combine\n                combined_probs = alpha * text_probs + (1 - alpha) * image_probs\n                predictions = torch.argmax(combined_probs, dim=1)\n                \n                all_predictions.extend(predictions.cpu().numpy())\n                all_labels.extend(labels.cpu().numpy())\n                all_text_probs.extend(text_probs.cpu().numpy())\n                all_image_probs.extend(image_probs.cpu().numpy())\n        \n        return (np.array(all_predictions), \n                np.array(all_labels),\n                np.array(all_text_probs),\n                np.array(all_image_probs))\n    \n    def optimize_alpha(self, val_dataloader, alphas=None):\n        \"\"\"\n        Find optimal alpha weight on validation set\n        \n        Args:\n            val_dataloader: Validation MultimodalDataset dataloader\n            alphas: List of alpha values to try\n            \n        Returns:\n            best_alpha, best_accuracy\n        \"\"\"\n        if alphas is None:\n            alphas = [0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n        \n        print(\"\\nOptimizing alpha weight...\")\n        print(\"Alpha | Accuracy\")\n        print(\"------|----------\")\n        \n        best_alpha = 0.5\n        best_acc = 0\n        \n        for alpha in alphas:\n            predictions, labels, _, _ = self.classify_batch(val_dataloader, alpha=alpha)\n            acc = accuracy_score(labels, predictions)\n            print(f\"{alpha:.1f}   | {acc:.4f}\")\n            \n            if acc > best_acc:\n                best_acc = acc\n                best_alpha = alpha\n        \n        print(f\"\\nBest alpha: {best_alpha} with accuracy: {best_acc:.4f}\")\n        return best_alpha, best_acc",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "7a09368f-c9fc-4c38-b792-645d870474a5",
      "cell_type": "code",
      "source": "\n# ============================================================================\n# PART 4: EVALUATION AND VISUALIZATION\n# ============================================================================\n\ndef plot_confusion_matrix(y_true, y_pred, class_names, title='Confusion Matrix'):\n    \"\"\"Plot confusion matrix\"\"\"\n    cm = confusion_matrix(y_true, y_pred)\n    \n    plt.figure(figsize=(8, 6))\n    sns.heatmap(cm, annot=True, fmt='g', cmap='Blues', cbar=False,\n                xticklabels=class_names, yticklabels=class_names)\n    plt.title(title)\n    plt.xlabel('Predicted Labels')\n    plt.ylabel('True Labels')\n    plt.tight_layout()\n    plt.savefig(title.replace(' ', '_').lower() + '.png', dpi=150, bbox_inches='tight')\n    plt.show()\n\n\ndef evaluate_models(text_model, image_model, multimodal_classifier,\n                   test_text_loader, test_image_loader, test_multimodal_loader,\n                   labels_test, class_names, alpha=0.5):\n    \"\"\"\n    Comprehensive evaluation of all three models\n    \"\"\"\n    print(\"\\n\" + \"=\"*70)\n    print(\"MODEL EVALUATION ON TEST SET\")\n    print(\"=\"*70)\n    \n    # Text model evaluation\n    print(\"\\n1. TEXT MODEL (DistilBERT)\")\n    print(\"-\" * 70)\n    text_predictions = predict_text(text_model, test_text_loader, DEVICE)\n    text_acc = accuracy_score(labels_test, text_predictions)\n    print(f\"Accuracy: {text_acc:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(labels_test, text_predictions, \n                                target_names=class_names))\n    plot_confusion_matrix(labels_test, text_predictions, class_names, \n                         'Text Model Confusion Matrix')\n    \n    # Image model evaluation\n    print(\"\\n2. IMAGE MODEL (ResNet18)\")\n    print(\"-\" * 70)\n    image_predictions = predict_image(image_model, test_image_loader, DEVICE)\n    image_acc = accuracy_score(labels_test, image_predictions)\n    print(f\"Accuracy: {image_acc:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(labels_test, image_predictions, \n                                target_names=class_names))\n    plot_confusion_matrix(labels_test, image_predictions, class_names,\n                         'Image Model Confusion Matrix')\n    \n    # Multimodal evaluation\n    print(f\"\\n3. MULTIMODAL MODEL (Combined, alpha={alpha})\")\n    print(\"-\" * 70)\n    multi_predictions, multi_labels, text_probs, image_probs = \\\n        multimodal_classifier.classify_batch(test_multimodal_loader, alpha=alpha)\n    multi_acc = accuracy_score(multi_labels, multi_predictions)\n    print(f\"Accuracy: {multi_acc:.4f}\")\n    print(\"\\nClassification Report:\")\n    print(classification_report(multi_labels, multi_predictions, \n                                target_names=class_names))\n    plot_confusion_matrix(multi_labels, multi_predictions, class_names,\n                         'Multimodal Model Confusion Matrix')\n    \n    # Summary comparison\n    print(\"\\n\" + \"=\"*70)\n    print(\"SUMMARY COMPARISON\")\n    print(\"=\"*70)\n    print(f\"Text Model:       {text_acc:.4f}\")\n    print(f\"Image Model:      {image_acc:.4f}\")\n    print(f\"Multimodal Model: {multi_acc:.4f}\")\n    print(f\"Improvement:      {multi_acc - max(text_acc, image_acc):.4f}\")\n    print(\"=\"*70)\n    \n    return {\n        'text_acc': text_acc,\n        'image_acc': image_acc,\n        'multimodal_acc': multi_acc,\n        'text_predictions': text_predictions,\n        'image_predictions': image_predictions,\n        'multimodal_predictions': multi_predictions\n    }\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "e470febe-743c-4c6e-aba2-c9fa3bd9c4a3",
      "cell_type": "code",
      "source": "\n# ============================================================================\n# PART 5: MAIN TRAINING AND EVALUATION PIPELINE\n# ============================================================================\n\ndef main():\n    \"\"\"Main training and evaluation pipeline\"\"\"\n    \n    print(\"=\"*70)\n    print(\"MULTIMODAL GARBAGE CLASSIFICATION SYSTEM\")\n    print(\"=\"*70)\n    \n    # ========================================================================\n    # STEP 1: LOAD AND PREPARE TEXT DATA\n    # ========================================================================\n    print(\"\\n[STEP 1] Loading text data from filenames...\")\n    text_train, labels_train_text, label_map = read_text_files_with_labels(TRAIN_PATH)\n    text_val, labels_val_text, _ = read_text_files_with_labels(VAL_PATH)\n    text_test, labels_test_text, _ = read_text_files_with_labels(TEST_PATH)\n    \n    class_names = sorted(label_map.keys())\n    print(f\"Classes: {class_names}\")\n    print(f\"Train samples: {len(text_train)}\")\n    print(f\"Val samples: {len(text_val)}\")\n    print(f\"Test samples: {len(text_test)}\")\n    \n    # ========================================================================\n    # STEP 2: TRAIN TEXT MODEL\n    # ========================================================================\n    print(\"\\n[STEP 2] Training text model (DistilBERT)...\")\n    \n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    \n    train_text_dataset = TextDataset(text_train, labels_train_text, tokenizer, TEXT_MAX_LEN)\n    val_text_dataset = TextDataset(text_val, labels_val_text, tokenizer, TEXT_MAX_LEN)\n    test_text_dataset = TextDataset(text_test, labels_test_text, tokenizer, TEXT_MAX_LEN)\n    \n    train_text_loader = DataLoader(train_text_dataset, batch_size=TEXT_BATCH_SIZE, shuffle=True)\n    val_text_loader = DataLoader(val_text_dataset, batch_size=TEXT_BATCH_SIZE, shuffle=False)\n    test_text_loader = DataLoader(test_text_dataset, batch_size=TEXT_BATCH_SIZE, shuffle=False)\n    \n    text_model = DistilBERTClassifier(num_classes=NUM_CLASSES).to(DEVICE)\n    text_optimizer = optim.Adam(text_model.parameters(), lr=TEXT_LR)\n    text_criterion = nn.CrossEntropyLoss()\n    \n    best_text_loss = 1e+10\n    for epoch in range(TEXT_EPOCHS):\n        train_loss = train_text_epoch(text_model, train_text_loader, \n                                      text_optimizer, text_criterion, DEVICE)\n        val_loss = evaluate_text_model(text_model, val_text_loader, \n                                       text_criterion, DEVICE)\n        print(f'Epoch {epoch+1}/{TEXT_EPOCHS} - Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}')\n        \n        if val_loss < best_text_loss:\n            best_text_loss = val_loss\n            torch.save(text_model.state_dict(), TEXT_MODEL_PATH)\n            print(\"  -> Saved best text model\")\n    \n    # Load best text model\n    text_model.load_state_dict(torch.load(TEXT_MODEL_PATH, weights_only=True))\n    print(f\"\\nText model training complete. Best val loss: {best_text_loss:.4f}\")\n    \n    # ========================================================================\n    # STEP 3: TRAIN IMAGE MODEL\n    # ========================================================================\n    print(\"\\n[STEP 3] Training image model (ResNet18)...\")\n    \n    # Image transforms\n    train_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    # Load image datasets\n    train_image_dataset = ImageFolder(root=TRAIN_PATH, transform=train_transform)\n    val_image_dataset = ImageFolder(root=VAL_PATH, transform=train_transform)\n    test_image_dataset = ImageFolder(root=TEST_PATH, transform=test_transform)\n    \n    train_image_loader = DataLoader(train_image_dataset, batch_size=IMAGE_BATCH_SIZE, \n                                    shuffle=True, num_workers=2)\n    val_image_loader = DataLoader(val_image_dataset, batch_size=IMAGE_BATCH_SIZE, \n                                  shuffle=False, num_workers=2)\n    test_image_loader = DataLoader(test_image_dataset, batch_size=IMAGE_BATCH_SIZE, \n                                   shuffle=False, num_workers=2)\n    \n    print(f\"Image dataset sizes - Train: {len(train_image_dataset)}, \"\n          f\"Val: {len(val_image_dataset)}, Test: {len(test_image_dataset)}\")\n    \n    # Create and train image model\n    image_model = GarbageImageModel(NUM_CLASSES, transfer=True).to(DEVICE)\n    image_criterion = nn.CrossEntropyLoss()\n    image_optimizer = torch.optim.AdamW(image_model.parameters(), lr=IMAGE_LR)\n    image_scheduler = ExponentialLR(image_optimizer, gamma=0.9)\n    \n    image_model = train_image_model(image_model, train_image_loader, val_image_loader,\n                                    image_criterion, image_optimizer, image_scheduler,\n                                    DEVICE, IMAGE_EPOCHS, IMAGE_MODEL_PATH)\n    \n    # Load best image model\n    image_model.load_state_dict(torch.load(IMAGE_MODEL_PATH, weights_only=True))\n    print(\"\\nImage model training complete.\")\n    \n    # ========================================================================\n    # STEP 4: CREATE MULTIMODAL CLASSIFIER\n    # ========================================================================\n    print(\"\\n[STEP 4] Creating multimodal classifier...\")\n    \n    # Create multimodal test dataset\n    test_multimodal_dataset = MultimodalDataset(test_image_dataset, text_test, \n                                                tokenizer, TEXT_MAX_LEN)\n    test_multimodal_loader = DataLoader(test_multimodal_dataset, \n                                       batch_size=IMAGE_BATCH_SIZE, \n                                       shuffle=False, num_workers=2)\n    \n    # Create multimodal validation dataset for alpha optimization\n    val_multimodal_dataset = MultimodalDataset(val_image_dataset, text_val, \n                                              tokenizer, TEXT_MAX_LEN)\n    val_multimodal_loader = DataLoader(val_multimodal_dataset, \n                                      batch_size=IMAGE_BATCH_SIZE, \n                                      shuffle=False, num_workers=2)\n    \n    multimodal_classifier = MultimodalGarbageClassifier(\n        text_model, image_model, tokenizer, test_transform, DEVICE, class_names\n    )\n    \n    # ========================================================================\n    # STEP 5: OPTIMIZE ALPHA WEIGHT\n    # ========================================================================\n    print(\"\\n[STEP 5] Optimizing alpha weight on validation set...\")\n    best_alpha, best_val_acc = multimodal_classifier.optimize_alpha(val_multimodal_loader)\n    \n    # ========================================================================\n    # STEP 6: EVALUATE ALL MODELS\n    # ========================================================================\n    print(\"\\n[STEP 6] Evaluating all models on test set...\")\n    results = evaluate_models(\n        text_model, image_model, multimodal_classifier,\n        test_text_loader, test_image_loader, test_multimodal_loader,\n        labels_test_text, class_names, alpha=best_alpha\n    )\n    \n    # ========================================================================\n    # STEP 7: DEMO - CLASSIFY SINGLE IMAGE\n    # ========================================================================\n    print(\"\\n[STEP 7] Demo - Single image classification\")\n    print(\"-\" * 70)\n    \n    # Get a sample from test set\n    sample_idx = 0\n    sample_path = test_image_dataset.samples[sample_idx][0]\n    sample_text = text_test[sample_idx]\n    sample_true_label = test_image_dataset.samples[sample_idx][1]\n    \n    print(f\"\\nSample Image Path: {sample_path}\")\n    print(f\"Sample Text: '{sample_text}'\")\n    print(f\"True Label: {class_names[sample_true_label]}\")\n    \n    pred_class, confidence, text_probs, image_probs = \\\n        multimodal_classifier.classify_from_path(sample_path, sample_text, \n                                                alpha=best_alpha, return_details=True)\n    \n    print(f\"\\nPredicted: {class_names[pred_class]} (confidence: {confidence:.3f})\")\n    print(\"\\nDetailed Probabilities:\")\n    print(\"Class        | Text Model | Image Model | Combined\")\n    print(\"-\" * 60)\n    for i, class_name in enumerate(class_names):\n        combined_prob = best_alpha * text_probs[i] + (1 - best_alpha) * image_probs[i]\n        print(f\"{class_name:12} | {text_probs[i]:10.3f} | {image_probs[i]:11.3f} | {combined_prob:8.3f}\")\n    \n    # ========================================================================\n    # STEP 8: SAVE RESULTS\n    # ========================================================================\n    print(\"\\n[STEP 8] Saving results...\")\n    \n    results_summary = {\n        'best_alpha': best_alpha,\n        'text_accuracy': results['text_acc'],\n        'image_accuracy': results['image_acc'],\n        'multimodal_accuracy': results['multimodal_acc'],\n        'class_names': class_names\n    }\n    \n    torch.save(results_summary, 'multimodal_results.pth')\n    print(\"Results saved to 'multimodal_results.pth'\")\n    \n    print(\"\\n\" + \"=\"*70)\n    print(\"PIPELINE COMPLETE!\")\n    print(\"=\"*70)\n    print(f\"\\nFinal Results:\")\n    print(f\"  Text Model Accuracy:       {results['text_acc']:.4f}\")\n    print(f\"  Image Model Accuracy:      {results['image_acc']:.4f}\")\n    print(f\"  Multimodal Model Accuracy: {results['multimodal_acc']:.4f}\")\n    print(f\"  Optimal Alpha:             {best_alpha:.2f}\")\n    print(f\"\\nModels saved:\")\n    print(f\"  - {TEXT_MODEL_PATH}\")\n    print(f\"  - {IMAGE_MODEL_PATH}\")\n    print(\"=\"*70)\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "3dbeb703-ec9c-443c-9dd0-65a00516bdb8",
      "cell_type": "code",
      "source": "\n# ============================================================================\n# BONUS: INFERENCE FUNCTION FOR NEW IMAGES\n# ============================================================================\n\ndef classify_new_image(image_path, text_description=\"\", \n                      text_model_path=TEXT_MODEL_PATH,\n                      image_model_path=IMAGE_MODEL_PATH,\n                      alpha=0.5):\n    \"\"\"\n    Classify a new garbage image with optional text description\n    \n    Args:\n        image_path: Path to the image\n        text_description: Optional text description\n        text_model_path: Path to trained text model\n        image_model_path: Path to trained image model\n        alpha: Weight for text vs image (default 0.5)\n    \n    Returns:\n        predicted_class_name, confidence\n    \"\"\"\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    class_names = ['Black', 'Blue', 'Green', 'TTR']\n    \n    # Load models\n    text_model = DistilBERTClassifier(num_classes=4).to(device)\n    text_model.load_state_dict(torch.load(text_model_path, weights_only=True))\n    \n    image_model = GarbageImageModel(4, transfer=True).to(device)\n    image_model.load_state_dict(torch.load(image_model_path, weights_only=True))\n    \n    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n    \n    test_transform = transforms.Compose([\n        transforms.Resize((224, 224)),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n    ])\n    \n    classifier = MultimodalGarbageClassifier(\n        text_model, image_model, tokenizer, test_transform, device, class_names\n    )\n    \n    pred_class, confidence = classifier.classify_from_path(\n        image_path, text_description, alpha=alpha\n    )\n    \n    return class_names[pred_class], confidence\n\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "id": "c6d9c080-e103-4c63-a214-ace753238cbc",
      "cell_type": "code",
      "source": "\n# ============================================================================\n# RUN THE PIPELINE\n# ============================================================================\n\nif __name__ == \"__main__\":\n    main()\n    \n    # Example of using the classifier on a new image:\n    # predicted_class, confidence = classify_new_image(\n    #     \"path/to/new/image.jpg\", \n    #     \"plastic bottle\", \n    #     alpha=0.5\n    # )\n    # print(f\"Predicted: {predicted_class} (confidence: {confidence:.3f})\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}