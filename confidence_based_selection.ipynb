{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e1efba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models, datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "# imports and check if cuda is avaiable\n",
    "\n",
    "NUM_CLASSES = 4\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b2fdc9",
   "metadata": {},
   "source": [
    "This note book loads the checkpoints for the best text model and best image model. The test data in the 15GB garbage_data set is fed to the model. The result is combined based on confidence selection rules. \n",
    "\n",
    "Note: We attempted to load the full saved model, but it resulted in lower accuracy compared to loading from checkpoints. In this notebook, the model architecture is explicitly defined, and the checkpoint weights are loaded into that architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "975989c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#redefined the architecture for text model\n",
    "class DistilBERTClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_classes):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.distilbert = DistilBertModel.from_pretrained(\n",
    "            \"distilbert-base-uncased\"\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.classifier = nn.Linear(\n",
    "            self.distilbert.config.hidden_size,\n",
    "            num_classes\n",
    "        )\n",
    "\n",
    "        # Freeze ALL DistilBERT layers initially\n",
    "        for param in self.distilbert.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "\n",
    "        outputs = self.distilbert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "\n",
    "        cls_output = outputs.last_hidden_state[:, 0]\n",
    "\n",
    "        x = self.dropout(cls_output)\n",
    "\n",
    "        return self.classifier(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81196e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the text model\n",
    "text_model = DistilBERTClassifier(NUM_CLASSES)\n",
    "\n",
    "checkpoint = torch.load(\"best_text_model.pth\", map_location=device)\n",
    "\n",
    "text_model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "text_model = text_model.to(device)\n",
    "text_model.eval()\n",
    "\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "2de36171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# redefined the image model architecture and load the image model\n",
    "# Create base EfficientNet model\n",
    "image_model = models.efficientnet_b2(weights=None)\n",
    "\n",
    "# Recreate the SAME classifier used during training\n",
    "in_features = image_model.classifier[1].in_features\n",
    "\n",
    "image_model.classifier = nn.Sequential(\n",
    "    nn.Linear(in_features, 512),\n",
    "    nn.BatchNorm1d(512),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "\n",
    "    nn.Linear(512, 128),\n",
    "    nn.BatchNorm1d(128),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.3),\n",
    "\n",
    "    nn.Linear(128, NUM_CLASSES),\n",
    ")\n",
    "\n",
    "# Load trained weights\n",
    "image_model.load_state_dict(\n",
    "    torch.load(\"best_image_model.pth\", map_location=device, weights_only=True)\n",
    ")\n",
    "\n",
    "# Move to device and set eval mode\n",
    "image_model = image_model.to(device)\n",
    "image_model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758c7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#define the dataset\n",
    "class MultimodalDataset(Dataset):\n",
    "\n",
    "    def __init__(self, image_dir, transform, tokenizer, max_len=24):\n",
    "        #ImageFolder automatically assigns labels\n",
    "        self.image_dataset = datasets.ImageFolder(image_dir, transform=transform)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #load image and label\n",
    "        image, label = self.image_dataset[idx]\n",
    "        # get the full file path\n",
    "        path = self.image_dataset.samples[idx][0]\n",
    "        # get file name e.g. plastic_bag.png\n",
    "        filename = os.path.basename(path)\n",
    "        # get item name e.g. plastic_bag\n",
    "        text = os.path.splitext(filename)[0]\n",
    "        text = text.replace('_', ' ')\n",
    "\n",
    "        # remove numerical value in name\n",
    "        text = re.sub(r'\\d+', '', text)\n",
    "\n",
    "        encoding = tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"text\": text,\n",
    "            \"input_ids\": encoding[\"input_ids\"].squeeze(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].squeeze(),\n",
    "            \"label\": label\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4a6116",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DATALOADER\n",
    "# image augmentation needed for the image model\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize((288, 288)),\n",
    "    transforms.CenterCrop(288),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        [0.485, 0.456, 0.406],\n",
    "        [0.229, 0.224, 0.225]\n",
    "    ),\n",
    "])\n",
    "\n",
    "TEST_PATH = r\"C:\\Users\\john2\\Desktop\\uofc\\617\\assignment2\\garbage_data\\garbage_data\\CVPR_2024_dataset_Test\"\n",
    "test_dataset = MultimodalDataset(\n",
    "    TEST_PATH,\n",
    "    transform_test,\n",
    "    tokenizer\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=32,\n",
    "    shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15da1b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PREDICTION\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "image_model.eval()\n",
    "text_model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for batch in test_loader:\n",
    "\n",
    "        images = batch[\"image\"].to(device)\n",
    "        input_ids = batch[\"input_ids\"].to(device)\n",
    "        attention_mask = batch[\"attention_mask\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "\n",
    "        # Get logits\n",
    "        image_logits = image_model(images)\n",
    "        text_logits = text_model(input_ids, attention_mask)\n",
    "\n",
    "        # Convert to probabilities\n",
    "        image_probs = F.softmax(image_logits, dim=1)\n",
    "        text_probs = F.softmax(text_logits, dim=1)\n",
    "\n",
    "        # Get confidence + prediction\n",
    "        image_conf, image_pred = torch.max(image_probs, dim=1)\n",
    "        text_conf, text_pred = torch.max(text_probs, dim=1)\n",
    "\n",
    "        # CONFIDENCE SELECTION\n",
    "        # Check which model has a higher confidence in its result and use that result\n",
    "        # For example image model output (0.7,0.2,0.1,0.1), text model output (0.6,0.2,0.2,0); the code will choose the result of the image model\n",
    "        use_image = image_conf > text_conf\n",
    "\n",
    "        final_pred = torch.where(use_image, image_pred, text_pred)\n",
    "\n",
    "        all_preds.extend(final_pred.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8f2da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confidence Fusion Accuracy: 0.8703379953379954\n"
     ]
    }
   ],
   "source": [
    "#ACCURACY\n",
    "accuracy = np.mean(np.array(all_preds) == np.array(all_labels))\n",
    "\n",
    "print(\"Confidence Fusion Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
